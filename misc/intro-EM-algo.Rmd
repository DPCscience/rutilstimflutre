---
title: "Introduction to the EM algorithm"
author: "TimothÃ©e Flutre (Inra)"
date: "`r format(Sys.time(), '%d/%m/%Y %H:%M:%S')`"
output:
  rmarkdown::html_vignette:
    toc: true
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: TRUE
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
vignette: >
  %\VignetteIndexEntry{Intro to EM algorithm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!--
setwd("~/src/rutilstimflutre/vignettes/")

library(devtools)
build_vignettes()

library(rmarkdown)
render("intro-EM-algo.Rmd", "html_document")
-->

# Preamble

License: [CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/)

References:

* [Stephens (2000)](http://www.stat.washington.edu/stephens/papers/tabstract.html), chapter 1 from his PhD thesis

* [Beal (2003)](http://www.cse.buffalo.edu/faculty/mbeal/thesis/), chapter 2 from his PhD thesis

* [Shalizi](http://www.stat.cmu.edu/~cshalizi/uADA/12/), lecture "Mixture Models, Latent Variables and the EM Algorithm"

* [Ghahramani (2012)](http://videolectures.net/mlss2012_ghahramani_graphical_models/), talk "Graphical Models"

* Robert and Casella (2009), book "Introducing Monte Carlo Methods with R"


# Motivation

A large part of any scientific activity is about measuring things, in other words collecting data, and it is not infrequent to collect heterogeneous data.
It seems therefore natural to say that the samples come from a mixture of clusters.
The aim is thus to recover from the data, i.e. to infer, (i) how many clusters there are, (ii) what are the features of these clusters, and (iii) from which cluster each statistical unit comes from.
In this document, we focus on points (ii) and (iii).


# Data

The data, generically noted $\mathcal{D}$, consists here in $N$ observations, gathered into the vector $\boldsymbol{x}$:

\[
\mathcal{D} = \{x_1, x_2, \ldots, x_N \}
\]

In this document, we suppose that each observation $x_i$ is univariate, i.e. a scalar.


# Assumption

Let us assume that the data are heterogeneous and that they can be partitioned into $K$ clusters (in this document, we suppose that $K$ is known).
This means that we expect a subset of the observations to come from cluster $k = 1$, another subset from cluster $k = 2$, and so on.


# Statistical model

Writing down the statistical model usually means starting by writing down the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) of the parameters.
Technically, we say that the observations were generated according to a [density function](http://en.wikipedia.org/wiki/Probability_density_function) $f$.
In this document, we will assume that this density is itself a mixture of densities, one per cluster.
Furthermore, observations from cluster $k$ are generated from a Normal distribution, $\mathcal{N}$, which density is here noted $\phi$, with mean $\mu_k$ and standard deviation $\sigma_k$.
Moreover, as we don't know for sure from which cluster a given observation comes from, we define the mixture weight $w_k$ (also called mixing proportion) to be the probability that any given observation comes from cluster $k$.

As a result, we have the following list of parameters: $\Theta = \{w_1,\ldots,w_K,\mu_1,\ldots,\mu_K,\sigma_1,\ldots,\sigma_K \}$.

Finally, we can write the likelihood, assuming that the observations are conditionally independent (conditionally on the parameters):

\begin{align}
\mathcal{L}(\Theta; \mathcal{D}) &= f(\mathcal{D} | \Theta) \\
 &= \prod_{i=1}^N f(x_i | \Theta) \\
 &= \prod_{i=1}^N \sum_{k=1}^{K} w_k \, \phi(x_i|\mu_k,\sigma_k) \\
 &= \prod_{i=1}^N \sum_{k=1}^{K} w_k \frac{1}{\sigma_k \sqrt{2\pi}} \exp \left( -\frac{1}{2} \left( \frac{x_i - \mu_k}{\sigma_k} \right) ^2 \right)
\end{align}

The constraints are: $\forall k, w_k \ge 0$ and $\sum_{k=1}^K w_k = 1$.


# Maximum likelihood

...


# Missing data

...


# EM algorithm

## Definition

...

## Theory

...

## Variational approximation

...

## Formulas for both steps

...


# Simulation

```{r}
simulUnivData <- function(K=2, N=100, gap=6){
  mus <- seq(0, gap*(K-1), gap)
  sigmas <- runif(n=K, min=0.5, max=1.5)
  tmp <- floor(rnorm(n=K-1, mean=floor(N/K), sd=5))
  ns <- c(tmp, N - sum(tmp))
  clusters <- as.factor(matrix(unlist(lapply(1:K, function(k){rep(k, ns[k])})),
                               ncol=1))
  obs <- matrix(unlist(lapply(1:K, function(k){
    rnorm(n=ns[k], mean=mus[k], sd=sigmas[k])
  })))
  new.order <- sample(1:N, N)
  obs <- obs[new.order]
  rownames(obs) <- NULL
  clusters <- clusters[new.order]
  return(list(obs=obs, clusters=clusters, mus=mus, sigmas=sigmas,
              mix.weights=ns/N))
}
```


# Inference

## E step

```{r}
Estep <- function(data, params){
  calcMembershipProbas(data, params$mus, params$sigmas, params$mix.weights)
}

calcMembershipProbas <- function(data, mus, sigmas, mix.weights){
  N <- length(data)
  K <- length(mus)
  tmp <- matrix(unlist(lapply(1:N, function(i){
    x <- data[i]
    norm.const <- sum(unlist(Map(function(mu, sigma, mix.weight){
      mix.weight * calcUnivNormDens(x, mu, sigma)}, mus, sigmas, mix.weights)))
    unlist(Map(function(mu, sigma, mix.weight){
      mix.weight * calcUnivNormDens(x, mu, sigma) / norm.const
    }, mus[-K], sigmas[-K], mix.weights[-K]))
  })), ncol=K-1, byrow=TRUE)
  membership.probas <- cbind(tmp, apply(tmp, 1, function(x){1 - sum(x)}))
  names(membership.probas) <- NULL
  return(membership.probas)
}

calcUnivNormDens <- function(x, mu, sigma){
  return( 1/(sigma * sqrt(2*pi)) * exp(-1/(2*sigma^2)*(x-mu)^2) )
}
```

## M step

```{r}
Mstep <- function(data, params, membership.probas){
  params.new <- list()
  sum.membership.probas <- apply(membership.probas, 2, sum)
  params.new$mus <- calcMlEstimMeans(data, membership.probas,
                                     sum.membership.probas)
  params.new$sigmas <- calcMlEstimStdDevs(data, params.new$mus,
                                          membership.probas,
                                          sum.membership.probas)
  params.new$mix.weights <- calcMlEstimMixWeights(data, membership.probas,
                                                  sum.membership.probas)
  return(params.new)
}

calcMlEstimMeans <- function(data, membership.probas, sum.membership.probas){
  K <- ncol(membership.probas)
  sapply(1:K, function(k){
    sum(unlist(Map("*", membership.probas[,k], data))) /
      sum.membership.probas[k]
  })
}

calcMlEstimStdDevs <- function(data, means, membership.probas,
                              sum.membership.probas){
  K <- ncol(membership.probas)
  sapply(1:K, function(k){
    sqrt(sum(unlist(Map(function(p.ki, x.i){
      p.ki * (x.i - means[k])^2
    }, membership.probas[,k], data))) /
    sum.membership.probas[k])
  })
}

calcMlEstimMixWeights <- function(data, membership.probas,
                                  sum.membership.probas){
  K <- ncol(membership.probas)
  sapply(1:K, function(k){
    1/length(data) * sum.membership.probas[k]
  })
}
```

## Log-likelihood

```{r}
logLikelihood <- function(data, mus, sigmas, mix.weights){
  loglik <- sum(sapply(data, function(x){
    log(sum(unlist(Map(function(mu, sigma, mix.weight){
      mix.weight * calcUnivNormDens(x, mu, sigma)
    }, mus, sigmas, mix.weights))))
  }))
  return(loglik)
}
```

## EM algorithm

```{r}
EMalgo <- function(data, params, threshold.convergence=10^(-2), nb.iter=10,
                   verbose=1){
  logliks <- vector()
  i <- 1
  if(verbose > 0) cat(paste("iter ", i, "\n", sep=""))
  membership.probas <- Estep(data, params)
  params <- Mstep(data, params, membership.probas)
  loglik <- logLikelihood(data, params$mus, params$sigmas,
                          params$mix.weights)
  logliks <- append(logliks, loglik)
  while(i < nb.iter){
    i <- i + 1
    if(verbose > 0) cat(paste("iter ", i, "\n", sep=""))
    membership.probas <- Estep(data, params)
    params <- Mstep(data, params, membership.probas)
    loglik <- logLikelihood(data, params$mus, params$sigmas,
                            params$mix.weights)
    if(loglik < logliks[length(logliks)]){
      msg <- paste("the log-likelihood is decreasing:",
                   loglik, "<", logliks[length(logliks)])
      stop(msg, call.=FALSE)
    }
    logliks <- append(logliks, loglik)
    if(abs(logliks[i] - logliks[i-1]) <= threshold.convergence)
      break
  }
  return(list(params=params, membership.probas=membership.probas,
              logliks=logliks, nb.iters=i))
}
```


# Evaluation

Simulate some data:

```{r}
K <- 3
N <- 300
simul <- simulUnivData(K, N)
data <- simul$obs
```

Run the EM algorithm:

```{r}
params0 <- list(mus=runif(n=K, min=min(data), max=max(data)),
                sigmas=rep(1, K),
                mix.weights=rep(1/K, K))
res <- EMalgo(data, params0, 10^(-3), 1000, 1)
```

Check its convergence:

```{r}
plot(res$logliks, xlab="iterations", ylab="log-likelihood",
     main="Convergence of the EM algorithm", type="b")
```

Plot the data along with the inferred densities:

```{r}
hist(data, breaks=30, freq=FALSE, col="grey", border="white",
     ylim=c(0,0.15), las=1,
     main="Histogram of data overlaid with densities inferred by EM")
rx <- seq(from=min(data), to=max(data), by=0.1)
ds <- lapply(1:K, function(k){
  dnorm(x=rx, mean=res$params$mus[k], sd=res$params$sigmas[k])
})
f <- sapply(1:length(rx), function(i){
  res$params$mix.weights[1] * ds[[1]][i] +
    res$params$mix.weights[2] * ds[[2]][i] +
    res$params$mix.weights[3] * ds[[3]][i]
})
lines(rx, f, col="red", lwd=2)
```


# Beyond

* many different distributions can be used besides the Normal

* the observations can be multivariate

* we can fit the model using Bayesian methods, e.g. MCMC or Variational Bayes

* we can try to estimate the number of components (K), e.g. by reversible-jump MCMC or via non-parametric Bayes

* there are issues, such as the fact that the EM can get stuck in a local maximum, or that the likelihood is invariant under permutations of the components' labels

* the parameters of each mixture component can depend on some known predictors, giving rise to mixture-of-experts models


# Appendix

```{r info}
print(sessionInfo(), locale=FALSE)
```
